<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Heap fragmentation or how my micro-benchmark went wrong</title>
  <meta name="description" content="Micro-benchmarking code always looks simple: a few variables, a small for loop and two std::chrono calls. I think this simplicity is an illusion.Micro-benchmarking is either complicated or inaccura..." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
  
  <meta property="og:site_name" content="Thoughts from a Wall Street developer" />
  <meta property="og:title" content="Heap fragmentation or how my micro-benchmark went wrong"/>
  
  <meta property="og:description" content="Micro-benchmarking code always looks simple: a few variables, a small for loop and two std::chrono calls. I think this simplicity is an illusion.Micro-benchmarking is either complicated or inaccura..." />
  
  <meta property="og:image" content="http://david-grs.github.io" />
  <meta property="og:url" content="http://david-grs.github.io/heap_fragmentation_micro_benchmark" >
  <meta property="og:type" content="blog" />
  <meta property="article:published_time" content="2016-12-28T16:56:00+01:00">

  <link rel="canonical" href="http://david-grs.github.io/heap_fragmentation_micro_benchmark"/>
  <link rel="shortcut icon" href="/assets/images/favicon.png" type="image/png"/>
  <link rel="stylesheet" href="//brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />

  <link rel="apple-touch-icon" sizes="57x57" href="/assets/favicon/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/favicon/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/favicon/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/favicon/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/favicon/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/favicon/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/favicon/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/favicon/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/assets/favicon/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/assets/favicon/favicon-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="/assets/favicon/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/assets/favicon/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/assets/favicon/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/assets/favicon/manifest.json">
  <link rel="mask-icon" href="/assets/favicon/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#00aba9">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#ffffff"> 
  
</head>

  <body itemscope itemtype="http://schema.org/Article">
    <!-- header start -->

<a href="http://david-grs.github.io" class="logo-readium"><span class="logo" style="background-image: url()"></span></a>

<!-- header end -->

    <main class="content" role="main">
      <article class="post">
        
        <div class="noarticleimage">
          <div class="post-meta">
            <h1 class="post-title">Heap fragmentation or how my micro-benchmark went wrong</h1>
            <div class="cf post-meta-text">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4 class="author-name" itemprop="author" itemscope itemtype="http://schema.org/Person">David Gross</h4>
              on
              <time datetime="2016-12-28T16:56:00+01:00">28 Dec 2016</time>
              <!-- , tagged on <span class="post-tag-">, <a href="/tag/"></a></span> -->
            </div>
          </div>
        </div>
        <br>
        <br>
        <br>
        
        <section class="post-content">
          <div class="post-reading">
            <span class="post-reading-time"></span> read
          </div>
          <a name="topofpage"></a>
          <p>Micro-benchmarking code always looks simple: a few variables, a small <em>for</em> loop and two <em>std::chrono</em> calls. I think this simplicity is an illusion.
Micro-benchmarking is either complicated or inaccurate. </p>

<p>You have to be careful that the compiler does not optimize out the code you are timing, accurately measure the time and know your time resolution,
all the caches are usually hot (as you loop over the same operation) and you will get a totally different cache hit rate than in production, a “production-like” set of inputs 
has to be generated — without affecting the resuts of the benchmark of course! —, on each iteration of the benchmark 
the state of your object is changing but should still allow the benchmark to continue, etc. It is really hard.</p>

<p>And this is why I usually don’t micro-benchmark: it is too hard for what it is ; you will get numbers and finally be disappointed or puzzled 
because you will not get the same latencies in production.</p>

<p>I prefer measuring the impact of my code changes directly in production — or in a production-like environment. The initial cost of setting up such timing 
system might be higher, but on the long term you get a lot of benefits from it. Lots of code changes can simply not be micro-benchmarked at all. In his 
talk <a href="https://www.youtube.com/watch?v=Qq_WaiwzOtI"><em>Optimization Tips - Mo’ Hustle Mo’ Problems</em></a>, Andrei Alexandrescu is presenting the speed gain of 
not inlining the destructor of an object — by simply defining an empty destructor in the source file instead of the default one. Good luck 
to micro-benchmark that, and if you actually do it <em>somehow</em>, I guess you will get the opposite result, as having the <em>inlined</em> call will save some instructions
and the instruction cache won’t be polluted.</p>

<p>This post is about my last adventure with a micro-benchmark. </p>

<h2 id="the-simplest-micro-benchmark">The simplest µ-benchmark</h2>
<p>This is a very simple — and the most boring I could find — micro-benchmark:</p>
<noscript><pre>void benchmark_unordered_set_emplace()
{
    std::unordered_set&lt;int&gt; uset;
    
    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i &lt; Iterations; ++i)
        uset.emplace(i, i);
    auto end = std::chrono::high_resolution_clock::now();
    
    std::cout &lt;&lt; Iterations &lt;&lt; &quot; iterations of unordered_set&lt;int&gt;::emplace() took &quot; 
              &lt;&lt; std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(end - start).count() 
              &lt;&lt; &quot;ms&quot; &lt;&lt; std::endl;
}</pre></noscript>
<script src="https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=awesome_benchmark.cc"> </script>

<p><br />
This gives us the output: <code>100000 iterations of unordered_set&lt;int&gt;::emplace() took 565ms</code>. Awesome. This is exactly what we wanted to know.</p>

<p>A few hours later, we added a lot of other small functions like this one. We are now measuring the time of the <em>emplace</em> operation for different containers,
with different contained type — to change their size and the cost of the move, copy, construction and destruction operations — and with different
number of elements in the container. We even added a layer of template because it looks better!</p>

<p>Our <em>benchmark()</em> function is now something like this:</p>
<noscript><pre>void benchmark()
{
    {
        std::mt19937 gen(seed);
        std::uniform_int_distribution&lt;&gt; rng;
        auto gen = [&amp;]() { return rng(gen); };
        
        benchmark_emplace&lt;std::unordered_set&lt;int&gt;&gt;(gen);
        benchmark_emplace&lt;std::set&lt;int&gt;&gt;(gen);
        benchmark_emplace&lt;boost::container::flat_set&lt;int&gt;&gt;(gen);
    }
  
    {
        // Foo is a medium size object...
        benchmark_emplace&lt;std::unordered_set&lt;Foo&gt;&gt;(&amp;Foo::Generate);
        benchmark_emplace&lt;std::set&lt;Foo&gt;&gt;(&amp;Foo::Generate);
        benchmark_emplace&lt;boost::container::flat_set&lt;Foo&gt;&gt;(&amp;Foo::Generate);

        // We also test big objects, different number of elements, and a lot of other fun stuff. 
    }
}</pre></noscript>
<script src="https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=even_more_awesome_benchmark.cc"> </script>

<p><br /></p>

<h2 id="the-tragedy">The tragedy</h2>
<p><em>At some point</em>, you decide to re-order the function calls — just for convenience. And boom. You didn’t 
expect <em>anything</em> to change because you have been at staring these numbers for hours… but your initial benchmark
is now <em>twice slower</em>: </p>

<p><code>100000 iterations of unordered_set&lt;int&gt;::emplace() took 1143ms</code></p>

<p>How come? You run again and get the same results. You checkout the previous commit and test again: it is fixed. How can a simple re-ordering of calls 
slow down a 5-lines function?</p>

<p>You decide to run the benchmark twice, as first benchmark and last one:</p>

<noscript><pre>void benchmark()
{
    std::cout &lt;&lt; &quot;first run&quot; &lt;&lt; benchmark_emplace&lt;std::unordered_set&lt;int&gt;&gt;(gen);
    
    {
        // a lot of other benchmarks (cf previous listing)...
    }
    
    std::cout &lt;&lt; &quot;second run&quot; &lt;&lt; benchmark_emplace&lt;std::unordered_set&lt;int&gt;&gt;(gen);
}</pre></noscript>
<script src="https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=benchmark_cmp.cc"> </script>

<p><br />
… which outputs:</p>

<pre><code>first run:   100000 iterations of unordered_set&lt;int&gt;::emplace() took 565ms
second run: 100000 iterations of unordered_set&lt;int&gt;::emplace() took 1143ms
</code></pre>

<h2 id="heap-fragmentation">Heap fragmentation</h2>
<p>As you guessed because it is the title of this article, the difference is due to the heap.</p>

<p>First of all, let’s confirm our hypothesis by actually measuring the time spent during the heap operations — <em>malloc</em>, <em>free</em> and <em>realloc</em>.</p>

<p>I updated for that an old <a href="https://github.com/david-grs/mtrace">C++ wrapper around the malloc hooks</a> that I developed a while ago to follow each memory allocation 
— by printing them. Here we don’t want to print them (there are millions!), but measure the time spent in these functions:</p>

<noscript><pre>template &lt;typename Container, typename KeyGenerator&gt;
void benchmark_emplace(KeyGenerator gen)
{
    Container c;
   
    // this setups our malloc/free/realloc hooks
    mtrace&lt;malloc_chrono&gt; mt;
  
    for (int i = 0; i &lt; Iterations; ++i)
        c.emplace(gen());
    
    std::cout &lt;&lt; &quot;time spent in malloc(): &quot; &lt;&lt; duration_cast&lt;milliseconds&gt;(mt.malloc_time()).count() &lt;&lt; &quot;ms\n&quot;
              &lt;&lt; &quot;time spent in free(): &quot; &lt;&lt; duration_cast&lt;milliseconds&gt;(mt.free_time()).count() &lt;&lt; &quot;ms\n&quot;
              &lt;&lt; &quot;time spent in realloc(): &quot; &lt;&lt; duration_cast&lt;milliseconds&gt;(mt.realloc_time()).count() &lt;&lt; &quot;ms&quot; &lt;&lt; std::endl;
}</pre></noscript>
<script src="https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=benchmark_mtrace.cc"> </script>

<p><br />
And this confirms what we thought:</p>

<pre><code>first run: benchmark_unordered_set_emplace() took 565ms
time spent in malloc(): 231ms
time spent in free(): 1ms
time spent in realloc(): 0ms

second run: benchmark_unordered_set_emplace() took 1143ms
time spent in malloc(): 916ms
time spent in free(): 94ms
time spent in realloc(): 0ms
</code></pre>

<p><br />
90% of the CPU time is spent in <em>malloc</em> and <em>free</em> on the second run of the benchmark! To go even more in depth, the GNU extension offers <em>malloc_info</em>. This 
function returns the list of bins — chunks that have been freed — by category (size, unsorted, fast). Let’s insert one call to <em>malloc_info</em> before 
each benchmark run:</p>

<pre><code>before the first run:
&lt;sizes&gt;
&lt;/sizes&gt;
&lt;total type="fast" count="0" size="0"/&gt;
&lt;total type="rest" count="0" size="0"/&gt;

before the second run:
&lt;sizes&gt;
&lt;size from="33" to="48" total="383999952" count="7999999"/&gt;
&lt;size from="49" to="64" total="64" count="1"/&gt;
&lt;size from="33" to="33" total="99" count="3"/&gt;
&lt;unsorted from="145" to="1009" total="1156000346" count="3999994"/&gt;
&lt;/sizes&gt;
&lt;total type="fast" count="8000000" size="384000016"/&gt;
&lt;total type="rest" count="3999997" size="1156000445"/&gt;
</code></pre>

<p><br />
What is very particular in this micro-benchmark is that we have millions of successive memory allocations: small allocated chunks from <em>std::unordered_set&lt;int&gt;::emplace</em>,
big ones from <em>boost::container::flat_set</em>, etc. Then, the benchmark is done, the block ends and the several containers go out of scope and are <em>destroyed</em>.</p>

<p>This leads to a very specific heap state: </p>

<ul>
  <li>Millions of chunks are successively destroyed</li>
  <li>They were also allocated successively, so quite likely to be contiguous and in the same pages</li>
  <li>These freed chunks represent almost all our memory allocations</li>
</ul>

<p>All freed chunks are merged together, the bins linked lists are cleared, the memory released. And this takes <em>forever</em>.</p>

<p>Remember our code:</p>
<noscript><pre>void benchmark()
{
    std::cout &lt;&lt; &quot;first run&quot; &lt;&lt; benchmark_emplace&lt;std::unordered_set&lt;int&gt;&gt;(gen);
    
    {
        // a lot of other benchmarks (cf previous listing)...
        // several destructors of big containers are called on the next line
    }
    
    std::cout &lt;&lt; &quot;second run&quot; &lt;&lt; benchmark_emplace&lt;std::unordered_set&lt;int&gt;&gt;(gen);
}</pre></noscript>
<script src="https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=benchmark_cmp2.cc"> </script>

<p><br />
If we introduce one <em>malloc(1)</em> call before our second run of the benchmark and run <em>malloc_info</em> again, we now get a heap in a clean state, and the time difference 
is gone. However, this is not a clean or portable solution.</p>

<p>The unfortunate part, is that there is no nice and clean solution. All the possible solutions I can think of are very dependent to the <em>malloc</em> implementation, and are made on several assumptions. 
For example, you could be simply rearrange the code to not destroy any of the benchmarked containers before 
the end of all benchmarks. This means that you only allocate, but don’t free any memory, which will prevent from the symptom we got earlier, but maybe not from another side effet. </p>

<p>In our simple case, we could simply change the code by removing the blocks and leave the inlined function calls in the single <em>benchmark</em> function:</p>
<noscript><pre>void benchmark()
{
    std::cout &lt;&lt; &quot;first run&quot; &lt;&lt; benchmark_emplace&lt;std::unordered_set&lt;int&gt;&gt;(gen);
    // a lot of other benchmarks (cf previous listing)...
    std::cout &lt;&lt; &quot;second run&quot; &lt;&lt; benchmark_emplace&lt;std::unordered_set&lt;int&gt;&gt;(gen);
}</pre></noscript>
<script src="https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=benchmark_cmp3.cc"> </script>

<p><br />
The only good solution — i.e. not based on any assumptions regarding the <em>malloc</em> implementation — would be to separate the different benchmarks between different applications
or application runs. </p>

<p>That’s all! I ran all my tests on my Ubuntu 14.04, based on the GLibC 2.19 (yes, this setup is getting old…). You can find a simplified version of the benchmark code 
<a href="https://github.com/david-grs/heap_frag">on my github</a>. Don’t be surprised if you get different results, everything in this article is implementation dependant.</p>

<p>Enjoy your holidays, and your micro-benchmarks!</p>


        </section>
        <footer class="post-footer">
          <section class="share">
            
              
            
          </section>
        </footer>
        <div class="bottom-teaser cf">
          <div class="isLeft">
            <h5 class="index-headline featured"><span>Written by</span></h5>
            <section class="author">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4>David Gross</h4>
              <p class="bio"></p>
              <hr>
              <p class="published">Published <time datetime="2016-12-28 16:56">28 Dec 2016</time></p>
            </section>
          </div>
          
          <div class="isRight">
            <h5 class="index-headline featured"><span>Supported by</span></h5>
            <footer class="site-footer">
              <section class="poweredby">Proudly published with <a href="http://jekyllrb.com"> Jekyll</a></section>
              <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> You should subscribe to my feed.</span></a>
              <div class="inner">
                <section class="copyright">All content copyright <a href="/">David Gross</a> &copy; 2017<br>All rights reserved.</section>
              </div>
            </footer>
          </div>
        </div>
        
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'david-grs'; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        
      </article>
    </main>
    <div class="bottom-closer">
      <div class="background-closer-image"  style="background-image: url(/assets/images/cover.jpg)">
        Image
      </div>
      <div class="inner">
        <h1 class="blog-title">Thoughts from a Wall Street developer</h1>
        <h2 class="blog-description">A blog about C++, with an emphasis on low-latency, performance measurements and system programming.
</h2>
        <a href="/" class="btn">Back to Overview</a>
      </div>
    </div>
    <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>


    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75603420-1', 'auto');
  ga('send', 'pageview');

</script>


  </body>
</html>
