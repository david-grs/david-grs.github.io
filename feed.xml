<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Thoughts from a Wall Street developer</title>
    <description>A blog about C++, with an emphasis on low-latency, performance measurements and system programming.
</description>
    <link>http://david-grs.github.io/</link>
    <atom:link href="http://david-grs.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 14 Sep 2017 23:50:50 +0200</pubDate>
    <lastBuildDate>Thu, 14 Sep 2017 23:50:50 +0200</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>C++ CI: Travis, CMake, GTest, Coveralls &amp; Appveyor</title>
        <description>&lt;p&gt;Last week, Jason Turner presented &lt;a href=&quot;https://www.youtube.com/watch?v=3ulKzD2cmSw&quot;&gt;an intro to Travis CI&lt;/a&gt;. I’ve never used it but have wanted to try for a while, so I
gave it a shot. Here is a short summary of this small adventure…&lt;/p&gt;

&lt;p&gt;First of all, what are these tools we are talking about?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Travis CI: a build farm for GNU/Linux and Mac OS X that also runs your unit tests&lt;/li&gt;
  &lt;li&gt;Coveralls: it works on the top of Travis CI, by generating coverage report (&lt;em&gt;lcov&lt;/em&gt; on Linux)&lt;/li&gt;
  &lt;li&gt;Appveyor: like Travis CI, but for Windows&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;h1 id=&quot;tldr&quot;&gt;TL;DR&lt;/h1&gt;
  &lt;p&gt;You can checkout all the configuration files on &lt;a href=&quot;https://github.com/david-grs/clang_travis_cmake_gtest_coveralls_example&quot;&gt;this github demo project&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To start, simply sign in with your GitHub account on &lt;a href=&quot;https://travis-ci.org/&quot;&gt;Travis CI&lt;/a&gt;, this will import all your repositories. From there, just enable one of them 
and add the following &lt;em&gt;.travis.yml&lt;/em&gt; to your repo:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;trusty&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;false&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;language&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cpp&lt;/span&gt;
  
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cmake .&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cmake --build .&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On &lt;em&gt;git push&lt;/em&gt;, it will automatically trigger your first Travis build. &lt;/p&gt;

&lt;h2 id=&quot;gcc-6--clang-5&quot;&gt;GCC 6 &amp;amp; Clang 5&lt;/h2&gt;
&lt;p&gt;By default, Travis CI is using a quite old version of GCC — 4.8, so no C++14 support. Switching to GCC 5 is quite simple and Jason explains in his video how to change the &lt;em&gt;.travis.yml&lt;/em&gt;. The one for GCC 6 is almost identical:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;trusty&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;false&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;language&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cpp&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;addons&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;apt&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;sources&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ubuntu-toolchain-r-test&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;packages&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;g++-6&lt;/span&gt; 

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;CXX=/usr/bin/g++-6 CC=/usr/bin/gcc-6 cmake .&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cmake --build .&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here, we are using the &lt;em&gt;apt&lt;/em&gt; add-on to install the &lt;em&gt;g++-6&lt;/em&gt; package. You can add any packages or libraries on which your project depends. Alternatively, 
you can also use the last &lt;em&gt;clang&lt;/em&gt; release to build your project:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;trusty&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;false&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;language&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cpp&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;addons&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;apt&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;sources&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;llvm-toolchain-trusty-5.0&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;packages&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;clang-5.0&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;CXX=/usr/bin/clang++-5.0 CC=/usr/bin/clang-5.0 cmake .&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cmake --build .&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;google-test&quot;&gt;Google Test&lt;/h2&gt;
&lt;p&gt;GTest is not handled by default — and we cannot blame Travis for that, but more Ubuntu that decided to stop distributing the library package. This means
that your CMake &lt;em&gt;FindPackage&lt;/em&gt; will fail:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;CMake Error at /usr/share/cmake-3.2/Modules/FindPackageHandleStandardArgs.cmake:138 (message)&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Could NOT find GTest (missing&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;GTEST_LIBRARY GTEST_MAIN_LIBRARY)&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;Call Stack (most recent call first)&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;/usr/share/cmake-3.2/Modules/FindPackageHandleStandardArgs.cmake:374 (_FPHSA_FAILURE_MESSAGE)&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;/usr/share/cmake-3.2/Modules/FindGTest.cmake:204 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)&lt;/span&gt;

  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;CMakeLists.txt:21 (find_package)&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;-- Configuring incomplete, errors occurred!&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I looked around and found that people solve this in various ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;With a pre-build bash script that installs the &lt;em&gt;libgtest-dev&lt;/em&gt; package, builds it and copies the libraries — this is quite hacky and fragile as it depends on the various system paths and library names.&lt;/li&gt;
  &lt;li&gt;With the GTest source in the repo — importing the whole source tree isn’t necessary&lt;/li&gt;
  &lt;li&gt;By adding GTest as a submodule — this is the way to go&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Add GTest as a submodule: 
&lt;code&gt;git submodule add git@github.com:google/googletest.git gtest&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then create a &lt;em&gt;gtest.cmake&lt;/em&gt; file with this content:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;set(GOOGLETEST_ROOT gtest/googletest CACHE STRING &amp;quot;Google Test source root&amp;quot;)&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;include_directories(SYSTEM&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;${PROJECT_SOURCE_DIR}/${GOOGLETEST_ROOT}&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;${PROJECT_SOURCE_DIR}/${GOOGLETEST_ROOT}/include&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;set(GOOGLETEST_SOURCES&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;${PROJECT_SOURCE_DIR}/${GOOGLETEST_ROOT}/src/gtest-all.cc&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;${PROJECT_SOURCE_DIR}/${GOOGLETEST_ROOT}/src/gtest_main.cc&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;foreach(_source ${GOOGLETEST_SOURCES})&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;set_source_files_properties(${_source} PROPERTIES GENERATED 1)&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;endforeach()&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;add_library(gtest ${GOOGLETEST_SOURCES})&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note the &lt;em&gt;SYSTEM&lt;/em&gt; keyword in &lt;em&gt;include_directories&lt;/em&gt;. Without it, you will get spammed on every build by all the warnings from GTest, especially if you build with &lt;em&gt;-Wall -Wextra etc&lt;/em&gt;. &lt;/p&gt;

&lt;p&gt;Last step, include it in your main CMakeLists.txt and link to &lt;em&gt;gtest&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;cmake_minimum_required(VERSION ...)&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;project(...)&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;include(gtest.cmake)&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# GTest needs threading support&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;find_package (Threads)&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;target_link_libraries(... gtest ${CMAKE_THREAD_LIBS_INIT})&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;coveralls&quot;&gt;Coveralls&lt;/h2&gt;
&lt;p&gt;As you did for Travis CI, you need to sign in with your GitHub account on &lt;a href=&quot;http://coveralls.io/&quot;&gt;Coveralls&lt;/a&gt; and enable your project. &lt;/p&gt;

&lt;p&gt;Now edit your &lt;em&gt;.travis.yml&lt;/em&gt; to install and run &lt;em&gt;cpp-coveralls&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;trusty&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;sudo&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;false&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;language&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cpp&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;addons&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;apt&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;sources&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ubuntu-toolchain-r-test&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;packages&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;g++-6&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;before_install&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;pip install --user cpp-coveralls&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;CXX=/usr/bin/g++-6 CC=/usr/bin/gcc-6 cmake -DCOVERAGE=1 .&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cmake --build .&lt;/span&gt; 
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;./tests&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;after_success&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;coveralls --root . -E &amp;quot;.*gtest.*&amp;quot; -E &amp;quot;.*CMakeFiles.*&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
A few things to note here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We excluded Google Test from the coverage with &lt;em&gt;-E “.*gtest.*”&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;-DCOVERAGE=1&lt;/em&gt; is passed to CMake — locally, you might not want to enable coverage &lt;/li&gt;
  &lt;li&gt;Don’t forget to run your unit tests if you want them in the coverage report&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, we need to enable the &lt;em&gt;gcda&lt;/em&gt; files generation — needed by &lt;em&gt;lcov&lt;/em&gt; — in our toolchain. This is done by adding
the &lt;em&gt;–coverage&lt;/em&gt; flag to both compiler and linker:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;SET(COVERAGE OFF CACHE BOOL &amp;quot;Coverage&amp;quot;)&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;if (COVERAGE)&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;target_compile_options(tests PRIVATE --coverage)&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;target_link_libraries(tests PRIVATE --coverage)&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;endif()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;appveyor&quot;&gt;Appveyor&lt;/h2&gt;
&lt;p&gt;As a Linux user, Appveyor was a bit tricky as I couldn’t build locally. Be sure to remember:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Appveyor will NOT clone your project with &lt;em&gt;git clone –recursive&lt;/em&gt;, so you need to run manually &lt;em&gt;git submodule update –init –recursive&lt;/em&gt; after the checkout&lt;/li&gt;
  &lt;li&gt;It is not possible to build directly with CMake — Appveyor expects a sln file, you can generate it via CMake, e.g. &lt;em&gt;cmake -G “Visual Studio 15 2017 Win64”&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;The last 1.8.0 release of GTest does not build with Visual Studio 2017 — you need to add the definition &lt;em&gt;GTEST_HAS_TR1_TUPLE=0&lt;/em&gt; to tell GTest not using ::tr1 stuff&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is my &lt;em&gt;appveyor.yml&lt;/em&gt; file:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;1.0.{build}&amp;#39;&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Visual Studio 2017&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;platform&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;x64&lt;/span&gt;
 
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;configuration&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Release&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Debug&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;install&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;git submodule update --init --recursive&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;before_build&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;cmake -G &amp;quot;Visual Studio 15 2017 Win64&amp;quot; .&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;$(APPVEYOR_BUILD_FOLDER)\$(APPVEYOR_PROJECT_NAME).sln&lt;/span&gt;

&lt;span class=&quot;l-Scalar-Plain&quot;&gt;test_script&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;%APPVEYOR_BUILD_FOLDER%\%CONFIGURATION%\tests.exe&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

</description>
        <pubDate>Thu, 14 Sep 2017 21:57:00 +0200</pubDate>
        <link>http://david-grs.github.io/cpp-clang-travis-cmake-gtest-coveralls-appveyor</link>
        <guid isPermaLink="true">http://david-grs.github.io/cpp-clang-travis-cmake-gtest-coveralls-appveyor</guid>
        
        
      </item>
    
      <item>
        <title>Heap fragmentation or how my micro-benchmark went wrong</title>
        <description>&lt;p&gt;Micro-benchmarking code always looks simple: a few variables, a small &lt;em&gt;for&lt;/em&gt; loop and two &lt;em&gt;std::chrono&lt;/em&gt; calls. I think this simplicity is an illusion.
Micro-benchmarking is either complicated or inaccurate. &lt;/p&gt;

&lt;p&gt;You have to be careful that the compiler does not optimize out the code you are timing, accurately measure the time and know your time resolution,
all the caches are usually hot (as you loop over the same operation) and you will get a totally different cache hit rate than in production, a “production-like” set of inputs 
has to be generated — without affecting the resuts of the benchmark of course! —, on each iteration of the benchmark 
the state of your object is changing but should still allow the benchmark to continue, etc. It is really hard.&lt;/p&gt;

&lt;p&gt;And this is why I usually don’t micro-benchmark: it is too hard for what it is ; you will get numbers and finally be disappointed or puzzled 
because you will not get the same latencies in production.&lt;/p&gt;

&lt;p&gt;I prefer measuring the impact of my code changes directly in production — or in a production-like environment. The initial cost of setting up such timing 
system might be higher, but on the long term you get a lot of benefits from it. Lots of code changes can simply not be micro-benchmarked at all. In his 
talk &lt;a href=&quot;https://www.youtube.com/watch?v=Qq_WaiwzOtI&quot;&gt;&lt;em&gt;Optimization Tips - Mo’ Hustle Mo’ Problems&lt;/em&gt;&lt;/a&gt;, Andrei Alexandrescu is presenting the speed gain of 
not inlining the destructor of an object — by simply defining an empty destructor in the source file instead of the default one. Good luck 
to micro-benchmark that, and if you actually do it &lt;em&gt;somehow&lt;/em&gt;, I guess you will get the opposite result, as having the &lt;em&gt;inlined&lt;/em&gt; call will save some instructions
and the instruction cache won’t be polluted.&lt;/p&gt;

&lt;p&gt;This post is about my last adventure with a micro-benchmark. &lt;/p&gt;

&lt;h2 id=&quot;the-simplest-micro-benchmark&quot;&gt;The simplest µ-benchmark&lt;/h2&gt;
&lt;p&gt;This is a very simple — and the most boring I could find — micro-benchmark:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;void benchmark_unordered_set_emplace()
{
    std::unordered_set&amp;lt;int&amp;gt; uset;
    
    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i &amp;lt; Iterations; ++i)
        uset.emplace(i, i);
    auto end = std::chrono::high_resolution_clock::now();
    
    std::cout &amp;lt;&amp;lt; Iterations &amp;lt;&amp;lt; &amp;quot; iterations of unordered_set&amp;lt;int&amp;gt;::emplace() took &amp;quot; 
              &amp;lt;&amp;lt; std::chrono::duration_cast&amp;lt;std::chrono::milliseconds&amp;gt;(end - start).count() 
              &amp;lt;&amp;lt; &amp;quot;ms&amp;quot; &amp;lt;&amp;lt; std::endl;
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=awesome_benchmark.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
This gives us the output: &lt;code&gt;100000 iterations of unordered_set&amp;lt;int&amp;gt;::emplace() took 565ms&lt;/code&gt;. Awesome. This is exactly what we wanted to know.&lt;/p&gt;

&lt;p&gt;A few hours later, we added a lot of other small functions like this one. We are now measuring the time of the &lt;em&gt;emplace&lt;/em&gt; operation for different containers,
with different contained type — to change their size and the cost of the move, copy, construction and destruction operations — and with different
number of elements in the container. We even added a layer of template because it looks better!&lt;/p&gt;

&lt;p&gt;Our &lt;em&gt;benchmark()&lt;/em&gt; function is now something like this:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;void benchmark()
{
    {
        std::mt19937 gen(seed);
        std::uniform_int_distribution&amp;lt;&amp;gt; rng;
        auto gen = [&amp;amp;]() { return rng(gen); };
        
        benchmark_emplace&amp;lt;std::unordered_set&amp;lt;int&amp;gt;&amp;gt;(gen);
        benchmark_emplace&amp;lt;std::set&amp;lt;int&amp;gt;&amp;gt;(gen);
        benchmark_emplace&amp;lt;boost::container::flat_set&amp;lt;int&amp;gt;&amp;gt;(gen);
    }
  
    {
        // Foo is a medium size object...
        benchmark_emplace&amp;lt;std::unordered_set&amp;lt;Foo&amp;gt;&amp;gt;(&amp;amp;Foo::Generate);
        benchmark_emplace&amp;lt;std::set&amp;lt;Foo&amp;gt;&amp;gt;(&amp;amp;Foo::Generate);
        benchmark_emplace&amp;lt;boost::container::flat_set&amp;lt;Foo&amp;gt;&amp;gt;(&amp;amp;Foo::Generate);

        // We also test big objects, different number of elements, and a lot of other fun stuff. 
    }
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=even_more_awesome_benchmark.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-tragedy&quot;&gt;The tragedy&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;At some point&lt;/em&gt;, you decide to re-order the function calls — just for convenience. And boom. You didn’t 
expect &lt;em&gt;anything&lt;/em&gt; to change because you have been at staring these numbers for hours… but your initial benchmark
is now &lt;em&gt;twice slower&lt;/em&gt;: &lt;/p&gt;

&lt;p&gt;&lt;code&gt;100000 iterations of unordered_set&amp;lt;int&amp;gt;::emplace() took 1143ms&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;How come? You run again and get the same results. You checkout the previous commit and test again: it is fixed. How can a simple re-ordering of calls 
slow down a 5-lines function?&lt;/p&gt;

&lt;p&gt;You decide to run the benchmark twice, as first benchmark and last one:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;void benchmark()
{
    std::cout &amp;lt;&amp;lt; &amp;quot;first run&amp;quot; &amp;lt;&amp;lt; benchmark_emplace&amp;lt;std::unordered_set&amp;lt;int&amp;gt;&amp;gt;(gen);
    
    {
        // a lot of other benchmarks (cf previous listing)...
    }
    
    std::cout &amp;lt;&amp;lt; &amp;quot;second run&amp;quot; &amp;lt;&amp;lt; benchmark_emplace&amp;lt;std::unordered_set&amp;lt;int&amp;gt;&amp;gt;(gen);
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=benchmark_cmp.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
… which outputs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;first run:   100000 iterations of unordered_set&amp;lt;int&amp;gt;::emplace() took 565ms
second run: 100000 iterations of unordered_set&amp;lt;int&amp;gt;::emplace() took 1143ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;heap-fragmentation&quot;&gt;Heap fragmentation&lt;/h2&gt;
&lt;p&gt;As you guessed because it is the title of this article, the difference is due to the heap.&lt;/p&gt;

&lt;p&gt;First of all, let’s confirm our hypothesis by actually measuring the time spent during the heap operations — &lt;em&gt;malloc&lt;/em&gt;, &lt;em&gt;free&lt;/em&gt; and &lt;em&gt;realloc&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I updated for that an old &lt;a href=&quot;https://github.com/david-grs/mtrace&quot;&gt;C++ wrapper around the malloc hooks&lt;/a&gt; that I developed a while ago to follow each memory allocation 
— by printing them. Here we don’t want to print them (there are millions!), but measure the time spent in these functions:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;template &amp;lt;typename Container, typename KeyGenerator&amp;gt;
void benchmark_emplace(KeyGenerator gen)
{
    Container c;
   
    // this setups our malloc/free/realloc hooks
    mtrace&amp;lt;malloc_chrono&amp;gt; mt;
  
    for (int i = 0; i &amp;lt; Iterations; ++i)
        c.emplace(gen());
    
    std::cout &amp;lt;&amp;lt; &amp;quot;time spent in malloc(): &amp;quot; &amp;lt;&amp;lt; duration_cast&amp;lt;milliseconds&amp;gt;(mt.malloc_time()).count() &amp;lt;&amp;lt; &amp;quot;ms\n&amp;quot;
              &amp;lt;&amp;lt; &amp;quot;time spent in free(): &amp;quot; &amp;lt;&amp;lt; duration_cast&amp;lt;milliseconds&amp;gt;(mt.free_time()).count() &amp;lt;&amp;lt; &amp;quot;ms\n&amp;quot;
              &amp;lt;&amp;lt; &amp;quot;time spent in realloc(): &amp;quot; &amp;lt;&amp;lt; duration_cast&amp;lt;milliseconds&amp;gt;(mt.realloc_time()).count() &amp;lt;&amp;lt; &amp;quot;ms&amp;quot; &amp;lt;&amp;lt; std::endl;
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=benchmark_mtrace.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
And this confirms what we thought:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;first run: benchmark_unordered_set_emplace() took 565ms
time spent in malloc(): 231ms
time spent in free(): 1ms
time spent in realloc(): 0ms

second run: benchmark_unordered_set_emplace() took 1143ms
time spent in malloc(): 916ms
time spent in free(): 94ms
time spent in realloc(): 0ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
90% of the CPU time is spent in &lt;em&gt;malloc&lt;/em&gt; and &lt;em&gt;free&lt;/em&gt; on the second run of the benchmark! To go even more in depth, the GNU extension offers &lt;em&gt;malloc_info&lt;/em&gt;. This 
function returns the list of bins — chunks that have been freed — by category (size, unsorted, fast). Let’s insert one call to &lt;em&gt;malloc_info&lt;/em&gt; before 
each benchmark run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;before the first run:
&amp;lt;sizes&amp;gt;
&amp;lt;/sizes&amp;gt;
&amp;lt;total type=&quot;fast&quot; count=&quot;0&quot; size=&quot;0&quot;/&amp;gt;
&amp;lt;total type=&quot;rest&quot; count=&quot;0&quot; size=&quot;0&quot;/&amp;gt;

before the second run:
&amp;lt;sizes&amp;gt;
&amp;lt;size from=&quot;33&quot; to=&quot;48&quot; total=&quot;383999952&quot; count=&quot;7999999&quot;/&amp;gt;
&amp;lt;size from=&quot;49&quot; to=&quot;64&quot; total=&quot;64&quot; count=&quot;1&quot;/&amp;gt;
&amp;lt;size from=&quot;33&quot; to=&quot;33&quot; total=&quot;99&quot; count=&quot;3&quot;/&amp;gt;
&amp;lt;unsorted from=&quot;145&quot; to=&quot;1009&quot; total=&quot;1156000346&quot; count=&quot;3999994&quot;/&amp;gt;
&amp;lt;/sizes&amp;gt;
&amp;lt;total type=&quot;fast&quot; count=&quot;8000000&quot; size=&quot;384000016&quot;/&amp;gt;
&amp;lt;total type=&quot;rest&quot; count=&quot;3999997&quot; size=&quot;1156000445&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
What is very particular in this micro-benchmark is that we have millions of successive memory allocations: small allocated chunks from &lt;em&gt;std::unordered_set&amp;lt;int&amp;gt;::emplace&lt;/em&gt;,
big ones from &lt;em&gt;boost::container::flat_set&lt;/em&gt;, etc. Then, the benchmark is done, the block ends and the several containers go out of scope and are &lt;em&gt;destroyed&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This leads to a very specific heap state: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Millions of chunks are successively destroyed&lt;/li&gt;
  &lt;li&gt;They were also allocated successively, so quite likely to be contiguous and in the same pages&lt;/li&gt;
  &lt;li&gt;These freed chunks represent almost all our memory allocations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All freed chunks are merged together, the bins linked lists are cleared, the memory released. And this takes &lt;em&gt;forever&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Remember our code:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;void benchmark()
{
    std::cout &amp;lt;&amp;lt; &amp;quot;first run&amp;quot; &amp;lt;&amp;lt; benchmark_emplace&amp;lt;std::unordered_set&amp;lt;int&amp;gt;&amp;gt;(gen);
    
    {
        // a lot of other benchmarks (cf previous listing)...
        // several destructors of big containers are called on the next line
    }
    
    std::cout &amp;lt;&amp;lt; &amp;quot;second run&amp;quot; &amp;lt;&amp;lt; benchmark_emplace&amp;lt;std::unordered_set&amp;lt;int&amp;gt;&amp;gt;(gen);
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=benchmark_cmp2.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
If we introduce one &lt;em&gt;malloc(1)&lt;/em&gt; call before our second run of the benchmark and run &lt;em&gt;malloc_info&lt;/em&gt; again, we now get a heap in a clean state, and the time difference 
is gone. However, this is not a clean or portable solution.&lt;/p&gt;

&lt;p&gt;The unfortunate part, is that there is no nice and clean solution. All the possible solutions I can think of are very dependent to the &lt;em&gt;malloc&lt;/em&gt; implementation, and are made on several assumptions. 
For example, you could be simply rearrange the code to not destroy any of the benchmarked containers before 
the end of all benchmarks. This means that you only allocate, but don’t free any memory, which will prevent from the symptom we got earlier, but maybe not from another side effet. &lt;/p&gt;

&lt;p&gt;In our simple case, we could simply change the code by removing the blocks and leave the inlined function calls in the single &lt;em&gt;benchmark&lt;/em&gt; function:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;void benchmark()
{
    std::cout &amp;lt;&amp;lt; &amp;quot;first run&amp;quot; &amp;lt;&amp;lt; benchmark_emplace&amp;lt;std::unordered_set&amp;lt;int&amp;gt;&amp;gt;(gen);
    // a lot of other benchmarks (cf previous listing)...
    std::cout &amp;lt;&amp;lt; &amp;quot;second run&amp;quot; &amp;lt;&amp;lt; benchmark_emplace&amp;lt;std::unordered_set&amp;lt;int&amp;gt;&amp;gt;(gen);
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/38b359aa9f20d7d27444c16c9e411855.js?file=benchmark_cmp3.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
The only good solution — i.e. not based on any assumptions regarding the &lt;em&gt;malloc&lt;/em&gt; implementation — would be to separate the different benchmarks between different applications
or application runs. &lt;/p&gt;

&lt;p&gt;That’s all! I ran all my tests on my Ubuntu 14.04, based on the GLibC 2.19 (yes, this setup is getting old…). You can find a simplified version of the benchmark code 
&lt;a href=&quot;https://github.com/david-grs/heap_frag&quot;&gt;on my github&lt;/a&gt;. Don’t be surprised if you get different results, everything in this article is implementation dependant.&lt;/p&gt;

&lt;p&gt;Enjoy your holidays, and your micro-benchmarks!&lt;/p&gt;

</description>
        <pubDate>Wed, 28 Dec 2016 16:56:00 +0100</pubDate>
        <link>http://david-grs.github.io/heap_fragmentation_micro_benchmark</link>
        <guid isPermaLink="true">http://david-grs.github.io/heap_fragmentation_micro_benchmark</guid>
        
        
      </item>
    
      <item>
        <title>Why you should use Boost.MultiIndex (Part II)</title>
        <description>&lt;p&gt;A few weeks ago, I posted the &lt;a href=&quot;/why_boost_multi_index_container-part1/&quot;&gt;first part&lt;/a&gt; of this article, where I explained the advantages 
of Boost.MultiIndex over the standard containers when you need to have multiple views on a set of data.&lt;/p&gt;

&lt;p&gt;In this second part, I would like to talk about the benefits you can get from using Boost.MultiIndex as a single-index hash table, as a replacement
of &lt;em&gt;std::unordered_map&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;One interesting and powerful aspect of Boost.MultiIndex is that it allows you to add an index of type &lt;em&gt;T&lt;/em&gt;, where &lt;em&gt;T&lt;/em&gt; is different from the stored type. And 
it is more frequent and useful that you could think.&lt;/p&gt;

&lt;h2 id=&quot;stdstring-as-a-key&quot;&gt;std::string as a key&lt;/h2&gt;
&lt;p&gt;Who never used a &lt;em&gt;std::unordered_map&amp;lt;std::string, X&amp;gt;&lt;/em&gt;? This is the most straightforward 
way to lookup for an object with a string: it is simple, relatively fast, and convenient as &lt;em&gt;std::hash&amp;lt;std::string&amp;gt;&lt;/em&gt; is 
provided by the standard library.&lt;/p&gt;

&lt;p&gt;However, what if you get another string type as input for your lookup? I already found myself in this situation multiple times:  &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;when reading from shared memory, a socket, or a device, you get a &lt;em&gt;const char*&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;when using APIs that provide their own string implementation, like Qt: even if your entire codebase is using &lt;em&gt;std::string&lt;/em&gt;, you will have some boundary zones where both types are used&lt;/li&gt;
  &lt;li&gt;when using third party libraries: you get most of the time a &lt;em&gt;const char *&lt;/em&gt; or &lt;em&gt;std::string&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;noscript&gt;&lt;pre&gt;struct stock
{
  std::string ref;
  double price;
  int volume;
};

struct message_handler
{
  void receive(const char* buffer, size_t size)  
  {
    // no choice here, lookup has to be on std::string
    // a std::string is constructed: memory allocation + bytes copy
    auto it = _stocks.find({buffer, size}); 
    // ...
  }

private:
  std::unordered_map&amp;lt;std::string, stock&amp;gt; _stocks;
};
&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/1f17bafa94b9596ec375532836fccd12.js?file=lookup_unordered_map_string.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
One would say that you could simply convert your string to &lt;em&gt;std::string&lt;/em&gt; before the lookup. However this might not be acceptable from a performance point of view, as it will perform a memory allocation. 
Nowadays, many STL implementations use “small string optimization* (SSO) — up to 15 bytes with g++ 6.2 and 22 chars with clang 3.9 — in order to avoid the memory allocation. Bytes are still 
copied though, and the performance of your system heavily relies on implementation details. &lt;/p&gt;

&lt;p&gt;The solution here is to use C++17’s &lt;a href=&quot;http://en.cppreference.com/w/cpp/experimental/basic_string_view&quot;&gt;&lt;em&gt;std::string_view&lt;/em&gt;&lt;/a&gt;. 
This is not possible with standard containers, though: with &lt;em&gt;std::unordered_map&lt;/em&gt; and &lt;em&gt;std::unordered_set&lt;/em&gt;, you cannot query on another type than &lt;em&gt;Key&lt;/em&gt; — and using a &lt;em&gt;std::unordered_map&amp;lt;std::string_view, X&amp;gt;&lt;/em&gt; 
is a bad choice, for a number of reasons I explain in the &lt;a href=&quot;#appendix&quot;&gt;appendix&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Easy task with Boost.MultiIndex: simlpy add a method that returns a string view and use it as index!&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;struct stock
{
  std::experimental::string_view ref_view() const { return ref; }
  // same attributes as before
};


struct message_handler
{
  void receive(const char* buffer, size_t size)  
  {
    // no memory allocation, no copy!
    std::experimental::string_view ref_view(buffer, size);
    auto it = _stocks.find(ref_view);
    // ...
  }

private:
  boost::multi_index_container&amp;lt;
    stock,
    indexed_by&amp;lt;
      hashed_unique&amp;lt;
        const_mem_fun&amp;lt;stock, std::experimental::string_view, &amp;amp;stock::ref_view&amp;gt;,
        std::hash&amp;lt;std::experimental::string_view&amp;gt;
      &amp;gt;
    &amp;gt;
  &amp;gt; _stocks;
};&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/1f17bafa94b9596ec375532836fccd12.js?file=lookup_mic_string_view.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
The magic is done on line 23, which defined the hashed unique index as &lt;code&gt;const_mem_fun&amp;lt;stock, std::experimental::string_view, &amp;amp;stock::ref_view&amp;gt;&lt;/code&gt;: a const member function of &lt;em&gt;stock&lt;/em&gt;, that returns
a string view, and can be found at &lt;em&gt;&amp;amp;stock::ref_view&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;unorderedsetltuniqueptrlttgtgt&quot;&gt;unordered_set&amp;lt;unique_ptr&amp;lt;T&amp;gt;&amp;gt;&lt;/h2&gt;
&lt;p&gt;Keeping a set of &lt;em&gt;unique_ptr&amp;lt;T&amp;gt;&lt;/em&gt; is less common, but sometimes convenient. Such pattern can be used if a class owns a collection of polymorphic &lt;em&gt;T&lt;/em&gt;, where this owner class does not 
know anything about the concrete type.&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;struct connection
{
  virtual ~connection() =default;
  
  virtual void close(const std::string&amp;amp; reason) =0;
};
 
struct node
{
  void new_connection(const parameters&amp;amp; params) { ... }

private:
  void on_error(connection&amp;amp; conn) 
  { 
    conn.close(&amp;quot;bye bye&amp;quot;);
    _connections.erase(&amp;amp;conn); // won&amp;#39;t compile: Key is a unique_ptr&amp;lt;connection&amp;gt;, not a connection*
  }
  
  std::unordered_set&amp;lt;std::unique_ptr&amp;lt;connection&amp;gt;&amp;gt; _connections;
};&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/1f17bafa94b9596ec375532836fccd12.js?file=unordered_set_unique_ptr.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
Note: &lt;em&gt;on_error&lt;/em&gt; is private. If it was public, this would be a poor API, as the client code would have a dangling reference the line right after the call to &lt;em&gt;node::on_error&lt;/em&gt;. &lt;em&gt;node&lt;/em&gt; owns 
and does not share &lt;em&gt;connection&lt;/em&gt; ; otherwise it could simply use a &lt;em&gt;std::unordered_set&amp;lt;std::shared_ptr&amp;lt;T&amp;gt;&amp;gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There are a few ways to solve this with unordered STL containers, but…&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;std::unordered_set&amp;lt;connection*&amp;gt;&lt;/code&gt;: you lose the advantages of RAII&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;std::unordered_map&amp;lt;connection*, std::unique_ptr&amp;lt;connection&amp;gt;&amp;gt;&lt;/code&gt;: ugly and under-performant solution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Boost.MultiIndex allows you here to store unique_ptr while being able to access it from raw pointers:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;struct node
{
  // ...

private:
  void on_error(connection&amp;amp; conn) 
  { 
    conn.close(&amp;quot;bye bye&amp;quot;);
    _connections.erase(&amp;amp;conn); // this line now builds!
  }
  
  using connection_ptr = std::unique_ptr&amp;lt;connection&amp;gt;;

  using connections = multi_index_container&amp;lt;
    connection_ptr,
    indexed_by&amp;lt;
      hashed_unique&amp;lt;
        const_mem_fun&amp;lt;connection_ptr, connection*, &amp;amp;connection_ptr::get&amp;gt;
      &amp;gt;
    &amp;gt;
  &amp;gt;;
  
  connections _connections;
};&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/1f17bafa94b9596ec375532836fccd12.js?file=boost_mic_unique_ptr.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;composite-keys&quot;&gt;composite keys&lt;/h2&gt;
&lt;p&gt;Composite keys is another quite common pattern when storing objects into associative containers. As the name implies, it allows to add an index composed by multiple fields. Let’s take a quick example.&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;struct session
{
  session(const std::string&amp;amp; username, const std::string&amp;amp; hostname) :
      _username(username),
      _hostname(hostname)
  {
    // ... 
  }

  const std::string&amp;amp; username() const { return _username; } 
  const std::string&amp;amp; hostname() const { return _hostname; } 
  
private:
  std::string _username;
  std::string _hostname;

  // more attributes ... 
  std::chrono::time_point&amp;lt;std::chrono::system_clock&amp;gt; _started_time;
  std::vector&amp;lt;process&amp;gt; _processes;
};&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/1f17bafa94b9596ec375532836fccd12.js?file=session.h&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
This class represents a session of a user on a particular machine. A user can only log once on a host, which allows us to uniquely identify a session by the association
of the &lt;em&gt;username&lt;/em&gt; and the &lt;em&gt;hostname&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;What are our options to do that with standard containers?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The most straightforward solution is to use a &lt;code&gt;std::unordered_map&amp;lt;std::pair&amp;lt;std::string, std::string&amp;gt;, session&amp;gt;&lt;/code&gt;, as you don’t have to provide any custom 
&lt;em&gt;std::hash&lt;/em&gt; nor &lt;em&gt;operator==&lt;/em&gt; implementations. I see a lot of problems with this solution though: it is bad from a performance point of view, because it uses four &lt;em&gt;std::string&lt;/em&gt; instances
while we only need two and it allocates two &lt;em&gt;std::string&lt;/em&gt; on each lookup. Also, the code is not very elegant and is error prone as you don’t know if &lt;em&gt;username&lt;/em&gt; is the 
first element of the pair or the second.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A better way to do it in my opinion is to have a &lt;code&gt;std::unordered_map&amp;lt;session_id, session&amp;gt;&lt;/code&gt;, with &lt;em&gt;session_id&lt;/em&gt; a &lt;em&gt;struct&lt;/em&gt; with two &lt;em&gt;std::string&lt;/em&gt;. This gets rid of a few drawbacks that we have in the solution 1, but requires a 
&lt;em&gt;std::hash&lt;/em&gt; specialization and &lt;em&gt;operator==&lt;/em&gt; definition. Not dramatic, but this raises more questions: how to implement a &lt;em&gt;good&lt;/em&gt; hash function that takes two &lt;em&gt;std::string&lt;/em&gt; as input?
I usually use &lt;a href=&quot;http://www.boost.org/doc/libs/1_62_0/doc/html/hash/combine.html&quot;&gt;Boost.Functional/Hash&lt;/a&gt; for that, but not every application can support Boost as a dependency. Apparently, 
a good start would be to use &lt;a href=&quot;http://stackoverflow.com/questions/17016175/c-unordered-map-using-a-custom-class-type-as-the-key&quot;&gt;a xor with some bit shifting&lt;/a&gt; on the two &lt;em&gt;std::string&lt;/em&gt; hashes, but 
who knows if this is really good?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A last solution I could think of is to use &lt;code&gt;std::unordered_set&amp;lt;session&amp;gt;&lt;/code&gt;, but it is still far from ideal: you have all the problems cited in the solution 2. The only advantage is that
you’re using less memory as you don’t have the two additional &lt;em&gt;std::string&lt;/em&gt; instances as key… but now you have to &lt;em&gt;const_cast&lt;/em&gt; all the time your stored objects, which can be annoying or simply not
desirable depending on your API. &lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As you can see, there is no simple and straightforward solution to that (simple!) problem. Here is what is looks like with Boost.MultiIndex:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;boost::multi_index_container&amp;lt;
  session,
  indexed_by&amp;lt;
    hashed_unique&amp;lt;
      composite_key&amp;lt;
        session,
        const_mem_fun&amp;lt;session, std::string, &amp;amp;session::username&amp;gt;,
        const_mem_fun&amp;lt;session, std::string, &amp;amp;session::hostname&amp;gt;
      &amp;gt;
    &amp;gt;
  &amp;gt;
&amp;gt; sessions;

sessions.insert({&amp;quot;foobar&amp;quot;, &amp;quot;localhost&amp;quot;});

auto it = sessions.find(boost::make_tuple(&amp;quot;foobar&amp;quot;, &amp;quot;localhost&amp;quot;));
std::cout &amp;lt;&amp;lt; it-&amp;gt;user_name &amp;lt;&amp;lt; std::endl;&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/1f17bafa94b9596ec375532836fccd12.js?file=boost_mic_composite_key.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
The &lt;em&gt;composite_key&lt;/em&gt; takes in this case two &lt;em&gt;const_mem_fun&lt;/em&gt; — which is the argument used to refer to const member functions. For the lookup, you will have to build a Boost.Tuple as a key.&lt;/p&gt;

&lt;p&gt;As I wrote in the first paragraph, building &lt;em&gt;std::string&lt;/em&gt; to do a lookup take a significant time. If you care enough about performance, here is the solution that is going to squeeze the
last bits and nanoseconds: &lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;boost::multi_index_container&amp;lt;
  session,
  indexed_by&amp;lt;
    hashed_unique&amp;lt;
      composite_key&amp;lt;
        session,
        const_mem_fun&amp;lt;session, std::experimental::string_view, &amp;amp;session::username&amp;gt;,
        const_mem_fun&amp;lt;session, std::experimental::string_view, &amp;amp;session::hostname&amp;gt;
      &amp;gt;,
      composite_key_hash&amp;lt;
        std::hash&amp;lt;std::experimental::string_view&amp;gt;,
        std::hash&amp;lt;std::experimental::string_view&amp;gt;
      &amp;gt;
    &amp;gt;
  &amp;gt;
&amp;gt; sessions;&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/1f17bafa94b9596ec375532836fccd12.js?file=boost_mic_composite_key_view.cc&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
Same approach as before: the composite key is still composed by the two methods of your class, that we modify of course in order to return a &lt;em&gt;std::string_view&lt;/em&gt; and not a &lt;em&gt;std::string&lt;/em&gt; anymore. The only
annoying bit is that we have to explicitly define the &lt;em&gt;composite_key_hash&lt;/em&gt; as Boost.MultiIndex doesn’t pick the &lt;em&gt;std::string_view&lt;/em&gt; specialization!&lt;/p&gt;

&lt;h2 id=&quot;a-nameappendixaappendix&quot;&gt;&lt;a name=&quot;appendix&quot;&gt;&lt;/a&gt;Appendix&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;Why using &lt;em&gt;std::unordered_map&amp;lt;std::string_view, X&amp;gt;&lt;/em&gt; is bad design choice?&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;It is a bad design because it is fragile and error-prone. The key is &lt;em&gt;const std::string_view&lt;/em&gt;, but as it is a view, it can actually change anytime. The only
acceptable scenario in my mind would be if &lt;em&gt;X&lt;/em&gt; owns the viewed string as &lt;em&gt;const std::string&lt;/em&gt;.
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;How do you insert an element in such container?&lt;/p&gt;

&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;m.emplace(x.str, x)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;std::experimental::string_view sv{x.str};&lt;/code&gt;&lt;br /&gt;
&lt;code&gt;m.emplace(sv, x.str); &lt;/code&gt; &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;m.emplace(x.str, std::move(x));&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;std::experimental::string_view sv{x.str};&lt;/code&gt;&lt;br /&gt;
&lt;code&gt;m.emplace(sv, std::move(x));&lt;/code&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;… and the answer is:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;is wrong because &lt;em&gt;x&lt;/em&gt; is going to be copied, while the key is pointing to &lt;em&gt;x.str&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;is wrong for the same reason&lt;/li&gt;
  &lt;li&gt;is wrong because there is no guarantee on the order of evaluation of arguments, thus &lt;em&gt;x&lt;/em&gt; could be moved before &lt;em&gt;x.str&lt;/em&gt; is evaluated, resulting in the construction of a string view that points to some undefined memory area&lt;/li&gt;
  &lt;li&gt;is correct: we construct the view, and move &lt;em&gt;x&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 23 Nov 2016 22:10:00 +0100</pubDate>
        <link>http://david-grs.github.io/why_boost_multi_index_container-part2</link>
        <guid isPermaLink="true">http://david-grs.github.io/why_boost_multi_index_container-part2</guid>
        
        
      </item>
    
      <item>
        <title>Why you should use Boost.MultiIndex (Part I)</title>
        <description>&lt;p&gt;Although Boost.MultiIndex is a pretty old library — introduced in Boost 1.32, released in 2004 — I found it rather unsung and underestimated
across the C++ community in comparison to other non-standard containers. &lt;/p&gt;

&lt;p&gt;In this article, split into multiple parts, I will highlight all the benefits you can get using &lt;em&gt;boost::multi_index_container&lt;/em&gt; instead of the standard 
containers: faster, cleaner and simpler code.&lt;/p&gt;

&lt;h2 id=&quot;multiple-views&quot;&gt;Multiple views&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;You have a set of struct composed by two integers x and y and do mainly two operations on it: &lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;em&gt;adding an entry&lt;/em&gt;&lt;/li&gt;
    &lt;li&gt;&lt;em&gt;iterating over the set in ascending order on x&lt;/em&gt;&lt;/li&gt;
    &lt;li&gt;&lt;em&gt;iterating over the set in ascending order on y&lt;/em&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is achievable quite easily with &lt;em&gt;boost::multi_index_container&lt;/em&gt;:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;namespace tags
{
    struct x_asc {};
    struct y_desc{};
    struct unordered {};
}

struct A
{
    A(int _x, int _y) :
      x(_x), y(_y)
    {)

    int x;
    int y;
};

using sorted_ints = boost::multi_index_container&amp;lt;
  A,                          // the stored object
  indexed_by&amp;lt;
    ordered_unique&amp;lt;           // first view
      tag&amp;lt;tags::x_asc&amp;gt;,       // tag used to access the view
      member&amp;lt;A, int, &amp;amp;A::x&amp;gt;,  // ordered on x... 
      std::less&amp;lt;int&amp;gt;          // ... by ascending order
    &amp;gt;,
    ordered_unique&amp;lt;           // second view
      tag&amp;lt;tags::y_asc&amp;gt;,       // tag used to access the view
      member&amp;lt;A, int, &amp;amp;A::y&amp;gt;,  // ordered on y... 
      std::less&amp;lt;int&amp;gt;          // ... by ascending order
    &amp;gt;
  &amp;gt;
&amp;gt;;&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/7519f9b37941626a673926e498317305.js?file=mic_ints_asc_desc.cpp&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
The “hard” part done, the usage of the container itself is trivial. Actually, most of the operations are identical to the ones you can find on a standard container. The only
difference is that you have to get a &lt;em&gt;view&lt;/em&gt; on the container for some of the operations:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;sorted_ints mic;

// insert(), emplace() and erase() can be performed directly on the container
mic.insert(A(2, 3));
mic.emplace(3, 4);

auto&amp;amp;&amp;amp; view = mic.get&amp;lt;tags::x_asc&amp;gt;();
auto it = view.find(3);

mic.erase(it);

for (auto&amp;amp;&amp;amp; a : view)
  std::cout &amp;lt;&amp;lt; &amp;quot;x=&amp;quot; &amp;lt;&amp;lt; a.x &amp;lt;&amp;lt; &amp;quot; y=&amp;quot; &amp;lt;&amp;lt; a.y &amp;lt;&amp;lt; std::endl;&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/7519f9b37941626a673926e498317305.js?file=mic_view.cpp&quot;&gt; &lt;/script&gt;

&lt;h2 id=&quot;the-unfortunate-alternative&quot;&gt;The unfortunate alternative&lt;/h2&gt;
&lt;p&gt;The alternative and unfortunately most common way to solve this problem is to maintain two &lt;em&gt;std::set&lt;/em&gt;, one using a comparator on &lt;em&gt;x&lt;/em&gt;, the other one on &lt;em&gt;y&lt;/em&gt;. 
However, there are few problems with this approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;it is &lt;em&gt;error prone&lt;/em&gt;: each operation has to be done on the two sets, and if you forget you will have a bad time&lt;/li&gt;
  &lt;li&gt;the &lt;em&gt;time efficiency&lt;/em&gt; is worse on insertion as you have the value twice — and even if the object is small it will be slower due to the memory allocations of the container&lt;/li&gt;
  &lt;li&gt;the &lt;em&gt;spatial efficiency&lt;/em&gt; will be worse&lt;/li&gt;
  &lt;li&gt;it can be &lt;em&gt;complicated&lt;/em&gt; if instead of storing twice the value you decide to store a pointer, reference or iterator in one of the two containers: they can be invalidated depending on the container and the operations performed on it&lt;/li&gt;
  &lt;li&gt;it is not very &lt;em&gt;elegant&lt;/em&gt; from the code perspective&lt;/li&gt;
  &lt;li&gt;if the constructor can throw, it will be even more fun, as you will have to handle exceptions properly and erase the previously inserted values&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To sum up, yes, you can isolate the two containers in a separate &lt;em&gt;class&lt;/em&gt;, wrap all the methods, deal with the exceptions to 
rollback the operation if something throws and code the unit tests. Good luck, though! And it will be slower than Boost.MultiIndex,
and there will be another few hundreds of lines of code to maintain. &lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;
&lt;p&gt;When we do C++, performance usually matters: how fast is this double-indexed &lt;em&gt;boost::multi_index_container&lt;/em&gt; compared to the two &lt;em&gt;std::set&lt;/em&gt;? &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speed_boost_multi_index_std_set.jpg&quot; alt=&quot;Speed comparisons between boost::multi_index_container with two ordered indexes and two std::set, time in ms, measured on 1e6 iterations.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we thought, the &lt;em&gt;multi_index_container&lt;/em&gt; is much faster on insertion (and removal). The solution using two &lt;em&gt;std::set&lt;/em&gt;’s forces us to duplicate all the operations that modify the container: on insertion
and deletion, both sets need to be updated. This is especially bad as these operations involve memory allocations ; you can then expect this time difference to increase with the size of the value stored.&lt;/p&gt;

&lt;p&gt;When doing a lookup or walking in ascending or descending order, performance are almost the same — as they are done only on &lt;em&gt;one&lt;/em&gt; std::set and not the two. &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The entire code of this benchmark is available on &lt;a href=&quot;https://github.com/david-grs/boost_multi_index_container/blob/master/integers.cc&quot;&gt;my github&lt;/a&gt; &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We see that the lookup is slightly slower, probably due to the overhead added by the multiple indexes. What can we do about that? Well, adding a third index, not &lt;em&gt;ordered&lt;/em&gt; this time, but &lt;em&gt;hashed&lt;/em&gt;! &lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;using sorted_ints = boost::multi_index_container&amp;lt;
  A,
  indexed_by&amp;lt;
    ordered_unique&amp;lt;
      tag&amp;lt;tags::x_asc&amp;gt;,
      member&amp;lt;A, int, &amp;amp;A::x&amp;gt;,
      std::less&amp;lt;int&amp;gt;
    &amp;gt;,
    ordered_unique&amp;lt;
      tag&amp;lt;tags::y_asc&amp;gt;,
      member&amp;lt;A, int, &amp;amp;A::y&amp;gt;,
      std::greater&amp;lt;int&amp;gt;
    &amp;gt;,
    hashed_unique&amp;lt;            // third view: unordered (hashed)
      tag&amp;lt;tags::unordered&amp;gt;,   // its tag
      identity&amp;lt;A&amp;gt;,            // we hash the entire A, not only x or y
      std::hash&amp;lt;A&amp;gt;
    &amp;gt;
  &amp;gt;
&amp;gt;;&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/7519f9b37941626a673926e498317305.js?file=mic_ints_asc_desc_hashed.cpp&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
… and the lookup is now ~5times faster. What is almost free with Boost.MultiIndex — adding an index — has definitely a cost in our alternative solution: if we add a &lt;em&gt;std::unorered_set&lt;/em&gt;,
the time of insertion and removal grows significantly (and the complexity of the code that manages this additional container too).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speed_boost_multi_index_std_set_and_unordered_set.jpg&quot; alt=&quot;Speed comparisons between boost::multi_index_container with three indexes (two ordered, one hashed) and two std::set plus std::unordered_set, time in ms, measured on 1e6 iterations.&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Behind the scene, &lt;em&gt;boost::multi_index_container&lt;/em&gt; uses a system of headers in order to do its job. Each node consists in the stored object, plus a sequence of headers, depending on the indexes
that are used.&lt;/p&gt;

&lt;p&gt;There is a specific header for each index type.&lt;/p&gt;

&lt;h3 id=&quot;ordered&quot;&gt;Ordered&lt;/h3&gt;
&lt;p&gt;The ordered index is implemented as a red-black tree, thus the header is composed by 3 pointers: one to the parent and the two children pointers. An interesting point here is that the color of the node 
is — on most platforms — encoded in the LSB of the parent pointer, resulting in a header of 24 bytes instead of 32 bytes (on a 64-bit systems).&lt;/p&gt;

&lt;p&gt;As none of the &lt;em&gt;std::map&lt;/em&gt; implementations I checked — libc++ and libstdc++ trunk (at the time of writing…) — performs such space optimization, this might explain the result we got in the previous 
benchmark, where the lookup was faster on  &lt;em&gt;boost::multi_index_container&lt;/em&gt; than &lt;em&gt;std::set&lt;/em&gt;. I actually don’t know &lt;em&gt;why&lt;/em&gt; they don’t perform such optimization, if anybody knows, don’t hesitate to post a comment!&lt;/p&gt;

&lt;h3 id=&quot;unordered&quot;&gt;Unordered&lt;/h3&gt;
&lt;p&gt;The implementation of the hash map requires two pointers for the bucket management.&lt;/p&gt;

&lt;p&gt;Again, &lt;em&gt;std::unordered_map&lt;/em&gt; (with GCC 6.2 on Linux x86-64) takes more space (3 pointers), which can explain why the lookup was ~20% slower. I didn’t dig into the code of &lt;em&gt;std::unordered_map&lt;/em&gt;, I only profiled its memory allocations.&lt;/p&gt;

&lt;h3 id=&quot;sequenced&quot;&gt;Sequenced&lt;/h3&gt;
&lt;p&gt;The sequenced index is implemented as a double-linked list: the header is composed by 2 pointers, one pointing to the next node, the other one to the previous node.&lt;/p&gt;

&lt;h3 id=&quot;random-access&quot;&gt;Random access&lt;/h3&gt;
&lt;p&gt;The random access index is implemented with an array of pointers to the nodes. In the node itself, the header is only composed by one pointer, which points back to its corresponding element in the main array of pointers.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;index type&lt;/th&gt;
      &lt;th&gt;associated header           &lt;/th&gt;
      &lt;th&gt;size on 64-bit systems&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ordered&lt;/td&gt;
      &lt;td&gt;3 pointers&lt;/td&gt;
      &lt;td&gt;24 bytes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;unordered (hashed)            &lt;/td&gt;
      &lt;td&gt;2 pointers&lt;/td&gt;
      &lt;td&gt;16 bytes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;sequenced&lt;/td&gt;
      &lt;td&gt;2 pointers&lt;/td&gt;
      &lt;td&gt;16 bytes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;random&lt;/td&gt;
      &lt;td&gt;1 pointer&lt;/td&gt;
      &lt;td&gt;8 bytes&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
Then, in the previous double-ordered and hashed container, the overhead was &lt;em&gt;24+24+16=64 bytes&lt;/em&gt;. If we take &lt;em&gt;sizeof(int) == 4 bytes&lt;/em&gt;, each node takes &lt;em&gt;72 bytes&lt;/em&gt; in memory.&lt;/p&gt;

&lt;p&gt;In the other solution, each instance of &lt;em&gt;std::set&lt;/em&gt; and &lt;em&gt;std::unordered_set&lt;/em&gt; uses &lt;em&gt;40 bytes&lt;/em&gt; (GCC 6.2, Linux, x86-64), resulting in a usage of &lt;em&gt;120 bytes&lt;/em&gt; per node.&lt;/p&gt;

&lt;p&gt;That’s all for now! In a second part I will explain why Boost.MultiIndex can help you even if you don’t use multiple indexes.&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Nov 2016 21:47:00 +0100</pubDate>
        <link>http://david-grs.github.io/why_boost_multi_index_container-part1</link>
        <guid isPermaLink="true">http://david-grs.github.io/why_boost_multi_index_container-part1</guid>
        
        
      </item>
    
      <item>
        <title>static_any: a low-latency stack-based Boost.Any</title>
        <description>&lt;p&gt;When writing some speed-critical code, I wanted to use &lt;em&gt;Boost.Any&lt;/em&gt; as an attribute in a struct. This struct
was small — 2 cache lines — and I knew that all the types that I would store in this &lt;em&gt;Any&lt;/em&gt;
would fit in 16bytes. &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I thought “trivial, let’s use Boost.Any with a stack-based allocator”!&lt;br /&gt;
… and then I got disappointed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Disappointed, because &lt;em&gt;Boost.Any&lt;/em&gt; does not offer the possibility to use a custom allocator. To be honest, that would 
not have solved the story anyway, as &lt;em&gt;Boost.Any&lt;/em&gt; does other things that slow down this container, 
but that was enough to start thinking about another — home-made — solution.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Talk is cheap. Show me the code.&lt;br /&gt;
&lt;a href=&quot;https://github.com/david-grs/static_any&quot;&gt;static_any project on my github&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;boostany-implementation&quot;&gt;Boost.Any implementation&lt;/h2&gt;
&lt;p&gt;So why was &lt;em&gt;Boost.Any&lt;/em&gt; too slow in my case ? And why was I looking for a stack-based allocator ?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Boost.Any&lt;/em&gt; is very simple: it erases the type via inheritance, and the few required operations, which are querying the type
and cloning the objects, are done via virtual methods. Here is a skeleton:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;  struct any
  {
      struct placeholder { ... };

      template &amp;lt;typename _T&amp;gt; 
      struct holder : public placeholder {  _T; ... }

      template &amp;lt;typename _T&amp;gt; 
      any(const _T&amp;amp; t) : m_placeholder(new holder&amp;lt;_T&amp;gt;(t)) {}

  private:
      holder* m_placeholder;
  };&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8b727b6d2e71361ffa730df4004b449c.js?file=boost_any_impl&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
My main issue regarding this implementation was not at all with the costs of calling virtual methods, but with
the memory layout that this would imply to my structure.&lt;/p&gt;

&lt;p&gt;I knew that my stored types were small, and I wanted them on the same &lt;strong&gt;cache line&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;To be more concrete, let’s take the following code: &lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;  struct foo
  {
      int i;
      long l;
      boost::any a;
  };
  
  foo f;
  f.a = 1234;&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8b727b6d2e71361ffa730df4004b449c.js?file=mem_layout&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
The variables &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;l&lt;/em&gt; are next to each other in memory, but the integer &lt;em&gt;1234&lt;/em&gt; will be somewhere else — due to
the heap-based implementation of &lt;em&gt;Boost.Any&lt;/em&gt; — and maybe even on another page.&lt;/p&gt;

&lt;p&gt;And of course, you get all the benefits when having these variables located in the same contiguous chunk of memory: better
data-locality means less memory cache misses and a better prefetching.&lt;/p&gt;

&lt;h2 id=&quot;the-benchmark&quot;&gt;The benchmark&lt;/h2&gt;
&lt;p&gt;As always when talking about speed, we want to see some numbers. So here you are — it is in nanoseconds, the lower the better:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/bench_any_boost_qvariant.png&quot; alt=&quot;Time spent on assignment and get operations in nanoseconds&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The reason &lt;em&gt;static_any&lt;/em&gt; is faster on assignment is mainly due to the fact that is does not do a memory allocation. On the &lt;em&gt;get&lt;/em&gt; operation,
it is due to virtual calls and other implementation details.&lt;/p&gt;

&lt;p&gt;But the main thing, the reason I wanted to have such generic container, i.e. a stack-based any, is not shown by these numbers. Because
when you benchmark a piece of code, you run this one in a loop, and you do not get any cache misses… These numbers are just for &lt;em&gt;raw
sppeed&lt;/em&gt; and most of the time, instructions per cycle for such operations won’t be your bottleneck, but memory can be one.&lt;/p&gt;

&lt;h2 id=&quot;staticany&quot;&gt;static_any&lt;/h2&gt;
&lt;p&gt;The usage is as simple as &lt;em&gt;Boost.Any&lt;/em&gt;:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;static_any&amp;lt;32&amp;gt; a = 1234;
int x = a.get&amp;lt;int&amp;gt;(); // returns 1234

bool bi = a.has&amp;lt;int&amp;gt;(); // returns true
bool bd = a.has&amp;lt;double&amp;gt;(); // returns false

double d = a.get&amp;lt;double&amp;gt;(); // throws!

a = std::string(&amp;quot;hello world&amp;quot;); // moved to a&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8b727b6d2e71361ffa730df4004b449c.js?file=static_any_example&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;At the beginning, I only implemented it for trivially copyable types, as I was only using this container with such objects. This super simple 
and even faster container — but unsafe, as there is no type checking at runtime — is in the same header &lt;em&gt;any.hpp&lt;/em&gt; under the name 
of &lt;em&gt;static_any_t&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Later, after few discussions with &lt;a href=&quot;https://github.com/maciekgajewski&quot;&gt;my workmate Maciek&lt;/a&gt;, he got the awesome idea about the &lt;em&gt;gateway function&lt;/em&gt;
that allows &lt;em&gt;static_any&lt;/em&gt; to go from the erased type — the vector of bytes that is used as underlying in &lt;em&gt;static_any&lt;/em&gt; — to the real
type T that is stored.&lt;/p&gt;

&lt;p&gt;I will describe that in a later post, meanwhile you can grab the code…&lt;/p&gt;

</description>
        <pubDate>Thu, 12 May 2016 21:48:00 +0200</pubDate>
        <link>http://david-grs.github.io/low_latency_stack_based_boost_any</link>
        <guid isPermaLink="true">http://david-grs.github.io/low_latency_stack_based_boost_any</guid>
        
        
      </item>
    
      <item>
        <title>TLS performance overhead and cost on GNU/Linux</title>
        <description>&lt;p&gt;When changing the implementation of a time clock, I had to switch from a static method — think about a classic
call to &lt;em&gt;clock_gettime()&lt;/em&gt; for example — to a per-thread model. Long story short, it was about a constant TSC that
was not so constant across different sockets. &lt;/p&gt;

&lt;p&gt;Per-thread and static variables: that sounds like a perfect case for Thread-Local Storage (TLS). However this time clock
was widely used in my project and was taking so far a couple of CPU cycles, and I wanted to be sure that the TLS would not add
a significant overhead.&lt;/p&gt;

&lt;p&gt;A quick look on Google only revealed an article from the Intel Developer Zone: &lt;a href=&quot;https://software.intel.com/en-us/blogs/2011/05/02/the-hidden-performance-cost-of-accessing-thread-local-variables&quot;&gt;the hidden performance cost of accessing 
thread-local variables&lt;/a&gt;.
Among pro-tips like “you simply have to buy a Intel©® VTune™ licence and don’t forget the Parallel Studio XE©®™ it
only costs 2k$”, the awesome screenshots or the comment section where someone says that the code is slow due to the TLS 
implementation of Windows while another one points out the code was running under Linux, I simply decided to &lt;em&gt;figure out myself&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So I did what I prefer: I profiled it.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h1 id=&quot;tldr&quot;&gt;TL;DR&lt;/h1&gt;
  &lt;p&gt;If you have &lt;strong&gt;statically linked&lt;/strong&gt; code, you can use a thread local variable like another one: there is only one TLS block and its
entire data is known at link-time, which allows the pre-calculation of all offsets.&lt;/p&gt;
  &lt;p&gt;&amp;nbsp;&lt;/p&gt;
  &lt;p&gt;If you are using &lt;strong&gt;dynamic modules&lt;/strong&gt;, there will be a lookup each time you access the thread local variable, but
you have some options in order to avoid it in speed-critical code.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;micro-benchmarking-the-access-of-tls-data&quot;&gt;µ-benchmarking the access of TLS data&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;I wrote a snippet that mimics the code I had in production: one was accessing a static variable, the other one a thread
local variable, through a static method of the class. &lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;#pragma once

struct Inc
{
    void operator()();
    static int m_counter;
};

struct IncTLS
{
    void operator()();
    static thread_local int m_counter;
};&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=inc.hpp&quot;&gt; &lt;/script&gt;

&lt;noscript&gt;&lt;pre&gt;#include &amp;quot;inc.hpp&amp;quot;

int Inc::m_counter = 0;
thread_local int IncTLS::m_counter = 0;

void Inc::operator()()    { ++m_counter; }
void IncTLS::operator()() { ++m_counter; }&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=inc.cpp&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
Note that the &lt;em&gt;thread_local&lt;/em&gt; keyword was introduced in C++11, and implemented in GCC 4.8 or newer. If you are using an older version, you can use the &lt;em&gt;__thread&lt;/em&gt; GCC keyword.&lt;/p&gt;

&lt;p&gt;To time my experiments,  I am using &lt;a href=&quot;https://github.com/david-grs/geiger&quot; title=&quot;Geiger, a benchmarking library in C++&quot;&gt;geiger&lt;/a&gt;, a benchmarking library I developed. 
geiger can read &lt;a href=&quot;https://perf.wiki.kernel.org/index.php/Main_Page&quot; title=&quot;Profiling hardware counters&quot;&gt;hardware performance counters&lt;/a&gt; — using the PAPI library — 
and is ideal to benchmark very short snippets of code.&lt;/p&gt;

&lt;p&gt;Here, I simply declare a benchmark suite with two tests. The first one accesses static data, and the second one thread local data. The suite is run twice: the first time, each test is run once, 
and the second time, each test is run one hundred times.&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;#include &amp;quot;inc.hpp&amp;quot;

#include &amp;lt;geiger/benchmark.h&amp;gt;
#include &amp;lt;vector&amp;gt;

int main(int argc, char **argv)
{
    geiger::init();
    geiger::suite&amp;lt;geiger::instr_profiler&amp;gt; s;

    Inc inc;
    IncTLS inc_tls;

    s.add(&amp;quot;static&amp;quot;, [&amp;amp;]()
          {
              inc();
          });

    s.add(&amp;quot;tls&amp;quot;, [&amp;amp;]()
          {
              inc_tls();
          });

    s.set_printer&amp;lt;geiger::printers::console&amp;gt;();
    s.run(1);
    s.run(100);

    return 0;
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=tls.cpp&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
Let’s build it with our favorite flags:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;cmake_minimum_required(VERSION 2.8)

set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} -Wall -g -O3 std=c++14&amp;quot;)

add_executable(tls tls.cpp inc.cpp)
target_link_libraries(tls geiger)&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=CMakelists.txt&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
The results are quite straightforward: no overhead at all. There is always a bit of jitter with hardware counters, so do not 
conclude we have more cycles with the TLS test, there is quite some variance here between several runs.&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;Test      Time (ns)   PAPI_TOT_INS    PAPI_TOT_CYC   PAPI_BR_MSP
----------------------------------------------------------------
static            4            994           1,699            12
tls               4            994           1,812            10&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=Output.txt&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
The assembly code also confirms it. I build with CLang 3.6 but I got the same with GCC 4.7:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;$ disass ./tls &amp;quot;Inc::operator()()&amp;quot;
Dump of assembler code for function Inc::operator()():
   0x0000000000404f40 &amp;lt;+0&amp;gt;:     inc    DWORD PTR [rip+0x202382]
   0x0000000000404f46 &amp;lt;+6&amp;gt;:     ret    
End of assembler dump.

$ disass ./tls &amp;quot;IncTLS::operator()()&amp;quot;
Dump of assembler code for function IncTLS::operator()():
   0x0000000000404f50 &amp;lt;+0&amp;gt;:     inc    DWORD PTR fs:0xfffffffffffffffc
   0x0000000000404f58 &amp;lt;+8&amp;gt;:     ret    
End of assembler dump.&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=Assembly.txt&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;NB&lt;/strong&gt;: the glibc is using the segment register FS for accessing the TLS&lt;/p&gt;

&lt;h1 id=&quot;how-about-tlsgetaddr-&quot;&gt;How about __tls_get_addr ?&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Apparently, all is good, I can use TLS as I want. But hang on… how about this function that was in the callstack on the 
Intel blog, &lt;em&gt;__tls_get_addr&lt;/em&gt; ?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;One thing that always scares me the most when I do micro benchmarks is about the reliability of their results. Indeed, it 
is very easy to screw up. You have to generate some &lt;em&gt;realistic&lt;/em&gt; inputs that will trigger the same behavior that you got
in production, without too many side effects. And from the D-cache to the BTB, everything is hot…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here, we have a sneaky case: the compilation is part of the environment of our benchmark. If the thread variable is in a dynamic
module, the linker cannot compute a direct reference to it and we get the much vaunted lookup!&lt;/p&gt;

&lt;p&gt;Let’s build it again, as a shared library this time:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;cmake_minimum_required(VERSION 2.8)

set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} -Wall -g -O3 std=c++14&amp;quot;)

add_library(inc SHARED inc.cpp)
add_executable(tls tls.cpp)
target_link_libraries(tls inc geiger)&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=CMakelists_shared.txt&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
There is in this case a difference ; the method that accesses the thread local variable takes twice more time. Please not that 
this overhead is still very small, as in this case we compare it to an extremely cheap operation…&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;Test      Time (ns)   PAPI_TOT_INS    PAPI_TOT_CYC   PAPI_BR_MSP
----------------------------------------------------------------
static            6          1,194           2,990            20
tls              12          2,694           3,568            15&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=Output2.txt&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
The assembly code shows the same difference:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;$ disass ./libinc.so &amp;quot;Inc::operator()()&amp;quot;
Dump of assembler code for function Inc::operator()():
   0x0000000000000870 &amp;lt;+0&amp;gt;:     mov    rax,QWORD PTR [rip+0x200781]
   0x0000000000000877 &amp;lt;+7&amp;gt;:     inc    DWORD PTR [rax]
   0x0000000000000879 &amp;lt;+9&amp;gt;:     ret    
End of assembler dump.

$ disass ./libinc.so &amp;quot;IncTLS::operator()()&amp;quot;
Dump of assembler code for function IncTLS::operator()():
   0x0000000000000880 &amp;lt;+0&amp;gt;:     push   rax
   0x0000000000000881 &amp;lt;+1&amp;gt;:     data32 lea rdi,[rip+0x20074f]
   0x0000000000000889 &amp;lt;+9&amp;gt;:     data32 data32 call 0x770 &amp;lt;__tls_get_addr@plt&amp;gt;
   0x0000000000000891 &amp;lt;+17&amp;gt;:    inc    DWORD PTR [rax]
   0x0000000000000893 &amp;lt;+19&amp;gt;:    pop    rax
   0x0000000000000894 &amp;lt;+20&amp;gt;:    ret    
End of assembler dump.&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=Assembly2.txt&quot;&gt; &lt;/script&gt;

&lt;h1 id=&quot;the-tls-implementation-in-elf&quot;&gt;The TLS implementation in ELF&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;As global and static variables are stored in &lt;em&gt;.data&lt;/em&gt; and &lt;em&gt;.bss&lt;/em&gt; sections, thread local variables get stored in equivalent 
&lt;em&gt;.tdata&lt;/em&gt; and &lt;em&gt;.tbss&lt;/em&gt; sections.&lt;/p&gt;

&lt;p&gt;Each thread has its on TCB — &lt;em&gt;Thread Control Block&lt;/em&gt; — that contains some metadata about the thread itself. One
metadata is the address of the DTV — Dynamic Thread Vector — which is a vector that contains pointers to 
thread local variables and other TLS blocks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tls_implementation.jpg&quot; alt=&quot;TLS implementation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When using statically linked code, everything is simpler as there is &lt;strong&gt;only one TLS block&lt;/strong&gt;. As the offsets to the thread local
variables are known at link-time, the compiler is able to generate a direct access to the thread local variable. &lt;/p&gt;

&lt;p&gt;But when you have dynamic modules, several TLS blocks are present. It is easier to see with the following simplified version 
of &lt;em&gt;__tls_get_addr&lt;/em&gt;. &lt;em&gt;offset&lt;/em&gt; is known at build time, then &lt;em&gt;module_id&lt;/em&gt; is the only variable, which is zero when using static
linking.&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;void* __tls_get_offset(size_t module_id, size_t offset)
{
    char* tls_block = dtv[thread_id][module_id];
    
    if (tls_block == UNALLOCATED_TLS_BLOCK)
        tls_block = dtv[thread_id][module_id] = allocate_tls(module_id);
    
    return tls_block + offset;
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=tls_get_offset.c&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
This is the actual implementation from the glibc 2.20:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;void *
__tls_get_addr (GET_ADDR_ARGS)
{
  dtv_t *dtv = THREAD_DTV (); 

  if (__glibc_unlikely (dtv[0].counter != GL(dl_tls_generation)))
    return update_get_addr (GET_ADDR_PARAM);

  void *p = dtv[GET_ADDR_MODULE].pointer.val;

  if (__glibc_unlikely (p == TLS_DTV_UNALLOCATED))
    return tls_get_addr_tail (GET_ADDR_PARAM, dtv, NULL);

  return (char *) p + GET_ADDR_OFFSET;
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=dl-tls.c&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
The ABI specific parts are implemented in sysdeps ; as for x86_64, we access the TCB from the segment register FS:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;/* Return the address of the dtv for the current thread.  */
# define THREAD_DTV() \
  ({ struct pthread *__pd;                                                    \
     THREAD_GETMEM (__pd, header.dtv); })


/* Read member of the thread descriptor directly.  */
# define THREAD_GETMEM(descr, member) \
  ({ __typeof (descr-&amp;gt;member) __value;                                        \
     if (sizeof (__value) == 1)                                               \
       asm volatile (&amp;quot;movb %%fs:%P2,%b0&amp;quot;                                      \
                     : &amp;quot;=q&amp;quot; (__value)                                         \
                     : &amp;quot;0&amp;quot; (0), &amp;quot;i&amp;quot; (offsetof (struct pthread, member)));     \
      ....
&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/david-grs/8f8f38b6b63216a97c5c.js?file=x86_64_tls.h&quot;&gt; &lt;/script&gt;

&lt;h1 id=&quot;so-what&quot;&gt;So what?&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Accessing thread local variables is cheap in both cases ; the lookup done by the glibc is very fast (and takes a constant time). 
But if your code is — or could be — loaded as a dynamic module and is speed-critical, then it can be worth spending
some time to &lt;strong&gt;avoid this lookup&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;One easy win could be to access the thread local data from the same block of code. The compiler will be smart enough to call only
once &lt;em&gt;__tls_get_addr&lt;/em&gt;. This could be done by, for example, &lt;strong&gt;inlining&lt;/strong&gt; the function that access the TLS. &lt;/p&gt;

&lt;p&gt;One could think about &lt;em&gt;keeping the reference&lt;/em&gt; of the thread local variable, but I found this solution very dodgy. You might have to prepare your arguments to pass the code review :) …&lt;/p&gt;

&lt;p&gt;More information about the TLS and its implementation on GNU/Linux in &lt;a href=&quot;https://www.akkadia.org/drepper/tls.pdf&quot; title=&quot;ELF Handling for TLS from Ulrich Drepper&quot;&gt;ELF Handling for TLS&lt;/a&gt; from Ulrich Drepper.&lt;/p&gt;

</description>
        <pubDate>Sun, 20 Mar 2016 19:03:20 +0100</pubDate>
        <link>http://david-grs.github.io/tls_performance_overhead_cost_linux</link>
        <guid isPermaLink="true">http://david-grs.github.io/tls_performance_overhead_cost_linux</guid>
        
        
      </item>
    
  </channel>
</rss>
